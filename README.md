# Paper-Reading

##  Better & Faster Large Language Models via Multi-token Prediction

大模型预训练：区别于传统的next-token prediction, 提出一种简单的Multi-token Prediction, 让模型一次性预测多个token，从而提高样本效率。在大规模模型的效果尤其显著。在代码生成、简短算法推理等任务上收益尤其明显。通过实验分析了最佳窗大小。

### 模型架构
提出一种简单的Multi-token Prediction架构，它使用共享模型主干trunk和多个独立的输出预测头head，在不增加训练时间和内存开销的情况下实现并行Multi-token Prediction。

#### 内存高效实现
训练多标记预测器的一大挑战是降低 GPU 内存利用率。因为在当前的 LLM 中，词汇量 V 远大于潜在表示的维度 d，因此，logit 向量成为 GPU 内存使用瓶颈，引入多个头可能带来显存对的爆炸，使用序列化的方式处理每个头的前向和反向过程：每个独立的输出头，在主干处累积梯度。为输出头 $f_i$ 创建了 logits（及其梯度），但它在继续到下一个输出头 $f_{i+1}$ 之前被释放，仅需要长期存储 d 维主干梯度 $∂Ln/∂fs$。

#### 推理
在推理期间，所提出的架构最基本的用途是使用下一个令牌预测头 Pθ(xt+1 | xt:1) 进行普通的下一个令牌自回归预测，同时丢弃所有其他的。然而，可以利用额外的输出头来通过自推测解码方法加速下一个令牌预测头的解码，例如块并行解码（Stern等，2018），以及使用类似美杜莎的树注意力的推测性解码（Cai等，2024）。
###  模型表现
可以有效提升大模型的样本效率，尤其对于大规模模型，130亿参数多token预测模型在HumanEval和MBPP代码生成任务上分别比同等规模的单token预测模型高出12%和17%.

### 学习全局模式
语言模型通常由教师强制训练，模型在预训练期间接收每个未来标记的真是标签。然而，在测试期间，生成是无引导和自回归的，从而导致错误累积，鼓励模型专注于在非常短期内进行良好的预测，但可能会忽略生成序列整体结构中的长期依赖性。

多token预测有助于模型学习全局模式，根据训练标记与其后继标记的相关程度隐式地为训练标记分配权重。

### 推理加速
多token预测可以结合自推测解码技术，实现3倍推理速度提升。

### 微调应用
预训练的多token预测模型在微调阶段的表现优异，在CodeContents代码生成任务上Pass@K由于单token预测模型。
### 最佳预测窗口
取决于输入数据分布，在代码生成任务上，4token预测窗口表现最佳。

作者对Multi-token Prediction有效给出的推测：认为这种方式消除了训练时teacher-forcing和 推理时自回归的数据分布差异，并且多头loss会使模型给连续的token分配更高的权重，从而预测更准确。






